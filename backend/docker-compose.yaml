version: '3.8'

services:
  rag_api:
    build: . # Build the image from the Dockerfile in the current directory
    container_name: gst_ai_api
    ports:
      - "8765:8765" # Map host port 8765 to container port 8765
    volumes:
      # Mount the current directory to /app in the container
      - ./data:/app/data:ro
      # Mount a volume for ChromaDB persistence
      - chroma_db_data:/app/chroma_db_persist
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      # Pass other env vars if not using .env file or defined in Dockerfile
      - PERSIST_DIRECTORY=/app/chroma_db_persist
      - DATA_PATH=/app/data
      - EMBEDDING_MODEL_NAME=${EMBEDDING_MODEL_NAME:-sentence-transformers/bge-base-en-v1.5}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.1}
    # # If  LLM/Embedding requires GPU access WITHIN the container:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1 # or 'all'
    #           capabilities: [gpu]
 
    depends_on:
      # Add dependencies if running Ollama or Chroma/Qdrant as separate services
      - chroma # Example if Chroma was a separate service
    networks:
      - rag_network

  # Example: Running Chroma as a separate service (more advanced setup)
  chroma:
    image: chromadb/chroma:latest # Use the official ChromaDB image
    container_name: gst_ai_chroma
    ports:
      - "8001:8000" # Expose Chroma's port if needed for direct access/UI
    volumes:
      - chroma_db_data:/chroma/chroma # Persist Chroma data
    networks:
      - rag_network
    # environment: # Add Chroma specific env vars if needed
    #   - ALLOW_RESET=true

  # Example: Running Ollama as a separate service (if NOT running on host)
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: knowledge_sphere_ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama # Persist downloaded models
  #   # GPU Configuration for Ollama container
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1 # or 'all'
  #             capabilities: [gpu]
  #   networks:
  #     - rag_network

volumes:
  chroma_db_data: # Define the named volume for Chroma persistence
  # ollama_data: # Define volume for Ollama model persistence

networks:
  rag_network: # Define the network
    driver: bridge